import json
import time
import os
import subprocess
import sys
from typing import List, Dict, Any, Optional, Tuple
import logging
from datasets import load_dataset
import numpy as np
import requests
import re
from collections import Counter
import string
from scipy import stats
import matplotlib.pyplot as plt
import seaborn as sns

# Additional imports for enhanced evaluation
try:
    from rouge_score import rouge_scorer
    ROUGE_AVAILABLE = True
except ImportError:
    print("Warning: rouge_score not available. Install with: pip install rouge-score")
    ROUGE_AVAILABLE = False

try:
    from bert_score import score as bert_score
    BERTSCORE_AVAILABLE = True
except ImportError:
    print("Warning: bert_score not available. Install with: pip install bert_score")
    BERTSCORE_AVAILABLE = False

# TIRESRAG-R1 imports - these need to be installed from the actual project
try:
    # These would be the actual TIRESRAG-R1 imports based on the project structure
    import torch
    from transformers import AutoTokenizer, AutoModelForCausalLM
    TIRESRAG_AVAILABLE = True
except ImportError:
    print("Warning: TIRESRAG-R1 dependencies not available")
    TIRESRAG_AVAILABLE = False

# Setup logging
logging.basicConfig(level=logging.INFO, format='%(asctime)s - %(levelname)s - %(message)s')
logger = logging.getLogger(__name__)

class TIRESRAGModel:
    """TIRESRAG-R1 model implementation for evaluation"""
    
    def __init__(self, 
                 model_path: str = "Qwen/Qwen2.5-7B-Instruct",  # Based on typical TIRESRAG models
                 retrieval_api_url: str = "http://localhost:8000/retrieve",  # From wiki_servish.sh
                 answer_reflection_url: str = "http://localhost:8001/reflect",  # From answer_reflection_reward.sh
                 thinking_reward_url: str = "http://localhost:8002/thinking",  # From sufficient_thinking_reward.sh
                 max_tokens: int = 512,
                 temperature: float = 0.0,
                 top_p: float = 1.0):
        """Initialize TIRESRAG-R1 model for evaluation"""
        
        self.model_path = model_path
        self.retrieval_api_url = retrieval_api_url
        self.answer_reflection_url = answer_reflection_url
        self.thinking_reward_url = thinking_reward_url
        self.max_tokens = max_tokens
        self.temperature = temperature
        self.top_p = top_p
        
        # Initialize model and tokenizer
        try:
            if TIRESRAG_AVAILABLE:
                self.tokenizer = AutoTokenizer.from_pretrained(model_path, trust_remote_code=True)
                self.model = AutoModelForCausalLM.from_pretrained(
                    model_path,
                    torch_dtype=torch.bfloat16,
                    device_map="auto",
                    trust_remote_code=True
                )
                self.model.eval()
                logger.info(f"TIRESRAG-R1 model loaded: {model_path}")
            else:
                self.tokenizer = None
                self.model = None
                logger.warning("TIRESRAG-R1 dependencies not available - using fallback mode")
                
        except Exception as e:
            logger.error(f"Failed to initialize TIRESRAG-R1 model: {e}")
            self.tokenizer = None
            self.model = None

    def retrieve_documents(self, query: str, top_k: int = 5) -> List[Dict]:
        """Retrieve documents using the TIRESRAG retrieval API"""
        try:
            response = requests.post(
                self.retrieval_api_url,
                json={"query": query, "top_k": top_k},
                timeout=30
            )
            if response.status_code == 200:
                return response.json().get("documents", [])
            else:
                logger.warning(f"Retrieval API returned status {response.status_code}")
                return []
        except Exception as e:
            logger.warning(f"Retrieval API call failed: {e}")
            return []

    def think_retrieve_reflect(self, question: str) -> Dict[str, Any]:
        """Implement the think-retrieve-reflect process"""
        start_time = time.time()
        
        try:
            # Step 1: Think - Generate initial reasoning
            think_prompt = f"""Question: {question}

Let me think step by step about this question before retrieving information:

Thinking:"""
            
            thinking = self.generate_text(think_prompt, max_tokens=200)
            
            # Step 2: Retrieve - Get relevant documents
            retrieved_docs = self.retrieve_documents(question)
            
            # Build context from retrieved documents
            context_parts = []
            for i, doc in enumerate(retrieved_docs[:3]):  # Use top 3 documents
                if isinstance(doc, dict):
                    title = doc.get('title', f'Document {i+1}')
                    content = doc.get('text', doc.get('content', ''))
                    context_parts.append(f"{title}: {content[:300]}...")
            
            context_text = "\n".join(context_parts) if context_parts else ""
            
            # Step 3: Generate answer with context
            if context_text:
                answer_prompt = f"""Question: {question}

My initial thinking: {thinking}

Retrieved information:
{context_text}

Based on my thinking and the retrieved information, here is my answer:"""
            else:
                answer_prompt = f"""Question: {question}

My thinking: {thinking}

Answer:"""
            
            answer = self.generate_text(answer_prompt, max_tokens=200)
            
            # Step 4: Reflect - Evaluate answer quality
            reflection = self.reflect_on_answer(question, answer, context_text)
            
            inference_time = time.time() - start_time
            
            return {
                'text': answer,
                'thinking': thinking,
                'retrieved_docs': retrieved_docs,
                'reflection': reflection,
                'tokens_generated': len(answer.split()) if answer else 0,
                'inference_time': inference_time,
                'uses_retrieval': len(retrieved_docs) > 0,
                'utility_score': self.extract_utility_score(answer),
                'is_relevant': self.extract_relevance(answer),
                'support_level': self.extract_support(answer, context_text)
            }
            
        except Exception as e:
            logger.error(f"Error in think-retrieve-reflect process: {e}")
            return {
                'text': f"[Error] Could not complete think-retrieve-reflect process: {str(e)}",
                'thinking': "",
                'retrieved_docs': [],
                'reflection': {"quality": 0.0, "sufficiency": 0.0},
                'tokens_generated': 0,
                'inference_time': time.time() - start_time,
                'uses_retrieval': False,
                'utility_score': 0,
                'is_relevant': False,
                'support_level': 'no_support'
            }

    def generate_text(self, prompt: str, max_tokens: int = 200) -> str:
        """Generate text using the TIRESRAG model"""
        if self.model is None or self.tokenizer is None:
            # Fallback mode
            return f"[Simulated response to: {prompt[:50]}...]"
        
        try:
            inputs = self.tokenizer(prompt, return_tensors="pt", truncation=True, max_length=2048)
            
            with torch.no_grad():
                outputs = self.model.generate(
                    inputs.input_ids.to(self.model.device),
                    max_new_tokens=max_tokens,
                    temperature=self.temperature,
                    top_p=self.top_p,
                    do_sample=self.temperature > 0,
                    pad_token_id=self.tokenizer.eos_token_id
                )
            
            # Decode only the new tokens
            new_tokens = outputs[0][inputs.input_ids.shape[1]:]
            response = self.tokenizer.decode(new_tokens, skip_special_tokens=True)
            
            return response.strip()
            
        except Exception as e:
            logger.error(f"Error generating text: {e}")
            return f"[Generation Error: {str(e)}]"

    def reflect_on_answer(self, question: str, answer: str, context: str) -> Dict[str, float]:
        """Use reflection API to evaluate answer quality"""
        try:
            reflection_data = {
                "question": question,
                "answer": answer,
                "context": context
            }
            
            response = requests.post(
                self.answer_reflection_url,
                json=reflection_data,
                timeout=30
            )
            
            if response.status_code == 200:
                return response.json()
            else:
                logger.warning(f"Reflection API returned status {response.status_code}")
                
        except Exception as e:
            logger.warning(f"Reflection API call failed: {e}")
        
        # Fallback reflection scoring
        return {
            "quality": 0.7 if len(answer) > 50 else 0.3,
            "sufficiency": 0.8 if context else 0.4,
            "thinking_quality": 0.6 if "because" in answer.lower() or "since" in answer.lower() else 0.4
        }

    def extract_utility_score(self, text: str) -> int:
        """Extract utility score based on response quality"""
        if not text or len(text.strip()) < 10:
            return 1
        elif "[Error]" in text or "[Simulated" in text:
            return 1
        elif len(text.strip()) < 30:
            return 2
        elif len(text.strip()) < 80:
            return 3
        elif len(text.strip()) < 150:
            return 4
        else:
            return 5

    def extract_relevance(self, text: str) -> bool:
        """Check if response is relevant"""
        error_indicators = ['[Error]', '[Simulated', 'could not', 'cannot complete']
        return not any(indicator in text for indicator in error_indicators)

    def extract_support(self, text: str, context: str) -> str:
        """Extract support level based on context usage"""
        if '[Error]' in text or '[Simulated' in text:
            return 'no_support'
        elif context and len(context.strip()) > 50:
            return 'fully_supported'
        elif context and len(context.strip()) > 0:
            return 'partially_supported'
        else:
            return 'no_support'

class TIRESRAGEvaluator:
    """Evaluator for TIRESRAG-R1 following the paper's methodology"""
    
    def __init__(self):
        if ROUGE_AVAILABLE:
            self.rouge_scorer = rouge_scorer.RougeScorer(['rouge1', 'rouge2', 'rougeL'], use_stemmer=True)
        else:
            self.rouge_scorer = None
    
    def normalize_answer(self, s):
        """Normalize answer for evaluation (from SQuAD evaluation)"""
        def remove_articles(text):
            regex = re.compile(r'\b(a|an|the)\b', re.IGNORECASE)
            return re.sub(regex, ' ', text)
        
        def white_space_fix(text):
            return ' '.join(text.split())
        
        def remove_punc(text):
            exclude = set(string.punctuation)
            return ''.join(ch for ch in text if ch not in exclude)
        
        def lower(text):
            return text.lower()
        
        return white_space_fix(remove_articles(remove_punc(lower(s))))
    
    def exact_match_score(self, prediction, ground_truth):
        """Compute exact match score"""
        return (self.normalize_answer(prediction) == self.normalize_answer(ground_truth))
    
    def f1_score(self, prediction, ground_truth):
        """Compute F1 score (token-level)"""
        pred_tokens = self.normalize_answer(prediction).split()
        gold_tokens = self.normalize_answer(ground_truth).split()
        
        if not pred_tokens and not gold_tokens:
            return 1.0
        if not pred_tokens or not gold_tokens:
            return 0.0
        
        common = Counter(pred_tokens) & Counter(gold_tokens)
        num_same = sum(common.values())
        
        if num_same == 0:
            return 0.0
        
        precision = 1.0 * num_same / len(pred_tokens)
        recall = 1.0 * num_same / len(gold_tokens)
        f1 = (2 * precision * recall) / (precision + recall)
        
        return f1

    def evaluate_multiple_answers(self, prediction, ground_truths):
        """Evaluate against multiple possible ground truth answers"""
        if not ground_truths:
            return {'em': 0.0, 'f1': 0.0}
        
        best_em = 0.0
        best_f1 = 0.0
        
        for gt in ground_truths:
            if not gt or not gt.strip():
                continue
                
            em = self.exact_match_score(prediction, gt)
            f1 = self.f1_score(prediction, gt)
            
            best_em = max(best_em, em)
            best_f1 = max(best_f1, f1)
        
        return {'em': best_em, 'f1': best_f1}

# Initialize global evaluator
evaluator = TIRESRAGEvaluator()

def load_dataset_with_backup(options, sample_size):
    """Load dataset with multiple backup options"""
    ds = None
    for option in options:
        try:
            if len(option) == 2:
                dataset_name, split = option
                logger.info(f"Attempting to load {dataset_name}...")
                ds = load_dataset(dataset_name, split=split)
            elif len(option) == 3:
                dataset_name, config, split = option
                logger.info(f"Attempting to load {dataset_name} with config {config}...")
                ds = load_dataset(dataset_name, config, split=split)
            elif len(option) == 4:
                dataset_name, config, split, trust_code = option
                logger.info(f"Attempting to load {dataset_name} with config {config}...")
                ds = load_dataset(dataset_name, config, split=split, trust_remote_code=trust_code)
            
            logger.info(f"Successfully loaded {dataset_name}")
            break
        except Exception as e:
            logger.warning(f"Failed to load {dataset_name}: {e}")
            continue
    
    if ds is None:
        return None
        
    if sample_size < len(ds):
        ds = ds.select(range(sample_size))
        
    return ds

def run_hotpot_qa_benchmark(model, sample_size: int = 100):
    """HotpotQA - Multi-hop reasoning benchmark"""
    logger.info(f"Running HotpotQA benchmark with {sample_size} samples...")
    
    hotpot_options = [
        ("hotpotqa/hotpot_qa", "distractor", "validation"),
        ("hotpotqa/hotpot_qa", "fullwiki", "validation"),
        ("hotpot_qa", "distractor", "validation"),
        ("hotpot_qa", "fullwiki", "validation")
    ]
    
    ds = load_dataset_with_backup(hotpot_options, sample_size)
    if ds is None:
        logger.error("Failed to load HotpotQA from all backup sources")
        return []
    
    logger.info(f"Using {len(ds)} samples from HotpotQA")
    results = []
    
    for i, item in enumerate(ds):
        try:
            question = item.get('question', '')
            answer = item.get('answer', '')
            context = item.get('context', [])
            supporting_facts = item.get('supporting_facts', [])
            level = item.get('level', 'unknown')
            type_question = item.get('type', 'unknown')
            
            # Use TIRESRAG think-retrieve-reflect process
            response = model.think_retrieve_reflect(question)
            
            # Evaluation
            if answer:
                scores = evaluator.evaluate_multiple_answers(response['text'], [answer])
            else:
                scores = {'em': 0.0, 'f1': 0.0}
            
            results.append({
                'dataset': 'hotpot_qa',
                'question': question,
                'response': response['text'],
                'thinking': response.get('thinking', ''),
                'reflection': response.get('reflection', {}),
                'ground_truth_answer': answer,
                'level': level,
                'type': type_question,
                'exact_match': scores['em'],
                'f1_score': scores['f1'],
                'inference_time': response['inference_time'],
                'tokens_generated': response['tokens_generated'],
                'utility_score': response['utility_score'],
                'is_relevant': response['is_relevant'],
                'support_level': response['support_level'],
                'uses_retrieval': response['uses_retrieval'],
                'num_context_paragraphs': len(context),
                'num_retrieved_docs': len(response.get('retrieved_docs', []))
            })
            
            if (i + 1) % 10 == 0:
                logger.info(f"Processed {i + 1}/{len(ds)} HotpotQA samples")
                
        except Exception as e:
            logger.error(f"Error processing HotpotQA item {i}: {e}")
            continue
    
    logger.info(f"HotpotQA benchmark completed with {len(results)} samples")
    return results

def run_natural_questions_benchmark(model, sample_size: int = 100):
    """Google Natural Questions - Real user questions benchmark"""
    logger.info(f"Running Natural Questions benchmark with {sample_size} samples...")
    
    nq_options = [
        ("google-research-datasets/natural_questions", "validation"),
        ("natural_questions", "validation")
    ]
    
    ds = load_dataset_with_backup(nq_options, sample_size)
    if ds is None:
        logger.error("Failed to load Natural Questions from all backup sources")
        return []
    
    logger.info(f"Using {len(ds)} samples from Natural Questions")
    results = []
    
    for i, item in enumerate(ds):
        try:
            question_text = item.get('question', {}).get('text', '') if isinstance(item.get('question'), dict) else item.get('question', '')
            
            # Handle different annotation formats
            annotations = item.get('annotations', [])
            answers = []
            if annotations:
                for annotation in annotations:
                    if isinstance(annotation, dict):
                        # Extract short answers
                        short_answers = annotation.get('short_answers', [])
                        for sa in short_answers:
                            if isinstance(sa, dict) and 'text' in sa:
                                answers.append(sa['text'])
                        
                        # Extract long answer if no short answers
                        if not answers:
                            long_answer = annotation.get('long_answer', {})
                            if isinstance(long_answer, dict) and 'candidate_index' in long_answer:
                                # This would need the document context to extract
                                pass
            
            if not answers:
                answers = [item.get('answer', '')]
            
            # Use TIRESRAG think-retrieve-reflect process
            response = model.think_retrieve_reflect(question_text)
            
            # Evaluation
            if answers and any(answers):
                scores = evaluator.evaluate_multiple_answers(response['text'], answers)
            else:
                scores = {'em': 0.0, 'f1': 0.0}
            
            results.append({
                'dataset': 'natural_questions',
                'question': question_text,
                'response': response['text'],
                'thinking': response.get('thinking', ''),
                'reflection': response.get('reflection', {}),
                'ground_truth_answer': answers,
                'exact_match': scores['em'],
                'f1_score': scores['f1'],
                'inference_time': response['inference_time'],
                'tokens_generated': response['tokens_generated'],
                'utility_score': response['utility_score'],
                'is_relevant': response['is_relevant'],
                'support_level': response['support_level'],
                'uses_retrieval': response['uses_retrieval'],
                'num_retrieved_docs': len(response.get('retrieved_docs', []))
            })
            
            if (i + 1) % 10 == 0:
                logger.info(f"Processed {i + 1}/{len(ds)} Natural Questions samples")
                
        except Exception as e:
            logger.error(f"Error processing Natural Questions item {i}: {e}")
            continue
    
    logger.info(f"Natural Questions benchmark completed with {len(results)} samples")
    return results

def run_triviaqa_benchmark(model, sample_size: int = 100):
    """TriviaQA - Trivia questions benchmark"""
    logger.info(f"Running TriviaQA benchmark with {sample_size} samples...")
    
    triviaqa_options = [
        ("trivia_qa", "rc.nocontext", "validation"),
        ("trivia_qa", "unfiltered.nocontext", "validation"),
        ("mandarjoshi/trivia_qa", "validation"),
        ("lucadiliello/triviaqa", "validation")
    ]
    
    ds = load_dataset_with_backup(triviaqa_options, sample_size)
    if ds is None:
        logger.error("Failed to load TriviaQA from all backup sources")
        return []
    
    logger.info(f"Using {len(ds)} samples from TriviaQA")
    results = []
    
    for i, item in enumerate(ds):
        try:
            question = item.get('question', '')
            answer = item.get('answer', {})
            
            # Handle different answer formats
            if isinstance(answer, dict):
                answers = answer.get('aliases', [])
                if 'value' in answer:
                    answers.append(answer['value'])
                if 'normalized_value' in answer:
                    answers.append(answer['normalized_value'])
            else:
                answers = [answer] if answer else []
            
            # Use TIRESRAG think-retrieve-reflect process
            response = model.think_retrieve_reflect(question)
            
            # Evaluation
            if answers:
                scores = evaluator.evaluate_multiple_answers(response['text'], answers)
            else:
                scores = {'em': 0.0, 'f1': 0.0}
            
            results.append({
                'dataset': 'triviaqa',
                'question': question,
                'response': response['text'],
                'thinking': response.get('thinking', ''),
                'reflection': response.get('reflection', {}),
                'ground_truth_answer': answers,
                'exact_match': scores['em'],
                'f1_score': scores['f1'],
                'inference_time': response['inference_time'],
                'tokens_generated': response['tokens_generated'],
                'utility_score': response['utility_score'],
                'is_relevant': response['is_relevant'],
                'support_level': response['support_level'],
                'uses_retrieval': response['uses_retrieval'],
                'num_retrieved_docs': len(response.get('retrieved_docs', []))
            })
            
            if (i + 1) % 10 == 0:
                logger.info(f"Processed {i + 1}/{len(ds)} TriviaQA samples")
                
        except Exception as e:
            logger.error(f"Error processing TriviaQA item {i}: {e}")
            continue
    
    logger.info(f"TriviaQA benchmark completed with {len(results)} samples")
    return results

def run_squad_v2_benchmark(model, sample_size: int = 100):
    """SQuAD v2 - Reading comprehension with unanswerable questions"""
    logger.info(f"Running SQuAD v2 benchmark with {sample_size} samples...")
    
    squad_options = [
        ("rajpurkar/squad_v2", "validation"),
        ("squad_v2", "validation")
    ]
    
    ds = load_dataset_with_backup(squad_options, sample_size)
    if ds is None:
        logger.error("Failed to load SQuAD v2 from all backup sources")
        return []
    
    logger.info(f"Using {len(ds)} samples from SQuAD v2")
    results = []
    
    for i, item in enumerate(ds):
        try:
            question = item.get('question', '')
            answers = item.get('answers', {})
            context = item.get('context', '')
            is_impossible = item.get('is_impossible', False)
            
            # Handle different answer formats
            if isinstance(answers, dict):
                answer_texts = answers.get('text', [])
            else:
                answer_texts = [answers] if answers else []
            
            # Use TIRESRAG think-retrieve-reflect process
            response = model.think_retrieve_reflect(question)
            
            # Evaluation
            if answer_texts and not is_impossible:
                scores = evaluator.evaluate_multiple_answers(response['text'], answer_texts)
            elif is_impossible:
                # For impossible questions, check if model correctly identifies as unanswerable
                impossible_indicators = ['no answer', 'cannot answer', 'impossible', 'unanswerable', 'not enough information']
                correctly_identified = any(indicator in response['text'].lower() for indicator in impossible_indicators)
                scores = {'em': 1.0 if correctly_identified else 0.0, 'f1': 1.0 if correctly_identified else 0.0}
            else:
                scores = {'em': 0.0, 'f1': 0.0}
            
            results.append({
                'dataset': 'squad_v2',
                'question': question,
                'context': context[:200] + '...' if len(context) > 200 else context,
                'response': response['text'],
                'thinking': response.get('thinking', ''),
                'reflection': response.get('reflection', {}),
                'ground_truth_answer': answer_texts,
                'is_impossible': is_impossible,
                'exact_match': scores['em'],
                'f1_score': scores['f1'],
                'inference_time': response['inference_time'],
                'tokens_generated': response['tokens_generated'],
                'utility_score': response['utility_score'],
                'is_relevant': response['is_relevant'],
                'support_level': response['support_level'],
                'uses_retrieval': response['uses_retrieval'],
                'num_retrieved_docs': len(response.get('retrieved_docs', []))
            })
            
            if (i + 1) % 10 == 0:
                logger.info(f"Processed {i + 1}/{len(ds)} SQuAD v2 samples")
                
        except Exception as e:
            logger.error(f"Error processing SQuAD v2 item {i}: {e}")
            continue
    
    logger.info(f"SQuAD v2 benchmark completed with {len(results)} samples")
    return results

def run_fever_benchmark(model, sample_size: int = 100):
    """FEVER - Fact Extraction and VERification benchmark"""
    logger.info(f"Running FEVER benchmark with {sample_size} samples...")
    
    fever_options = [
        ("fever/fever", "v1.0", "paper_dev"),
        ("fever/fever", "v1.0", "paper_test"),
        ("fever/fever", "v2.0", "validation"),
        ("fever/fever", "validation"),
        ("fever", "validation"),
        ("BeIR/fever", "validation"),
        ("mteb/fever", "validation")
    ]
    
    ds = load_dataset_with_backup(fever_options, sample_size)
    if ds is None:
        logger.error("Failed to load FEVER from all backup sources")
        return []
    
    logger.info(f"Using {len(ds)} samples from FEVER")
    results = []
    
    for i, item in enumerate(ds):
        try:
            claim = item.get('claim', '')
            label = item.get('label', '')
            evidence = item.get('evidence', [])
            
            # Convert FEVER task to QA format
            question = f"Is this claim true or false? Claim: {claim}"
            
            # Use TIRESRAG think-retrieve-reflect process
            response = model.think_retrieve_reflect(question)
            
            # Evaluation for fact verification
            # Check if model correctly identifies claim as SUPPORTED, REFUTED, or NOTENOUGHINFO
            response_lower = response['text'].lower()
            predicted_label = 'NOTENOUGHINFO'  # default
            
            if any(word in response_lower for word in ['true', 'correct', 'supported', 'verify', 'confirmed']):
                predicted_label = 'SUPPORTED'
            elif any(word in response_lower for word in ['false', 'incorrect', 'refuted', 'wrong', 'disputed']):
                predicted_label = 'REFUTED'
            elif any(word in response_lower for word in ['not enough', 'insufficient', 'unclear', 'cannot determine']):
                predicted_label = 'NOTENOUGHINFO'
            
            # Calculate accuracy
            label_accuracy = 1.0 if predicted_label == label else 0.0
            
            # For FEVER, we use label accuracy instead of EM/F1
            results.append({
                'dataset': 'fever',
                'question': question,
                'claim': claim,
                'response': response['text'],
                'thinking': response.get('thinking', ''),
                'reflection': response.get('reflection', {}),
                'ground_truth_label': label,
                'predicted_label': predicted_label,
                'label_accuracy': label_accuracy,
                'exact_match': label_accuracy,  # Using label accuracy as EM equivalent
                'f1_score': label_accuracy,     # Using label accuracy as F1 equivalent
                'inference_time': response['inference_time'],
                'tokens_generated': response['tokens_generated'],
                'utility_score': response['utility_score'],
                'is_relevant': response['is_relevant'],
                'support_level': response['support_level'],
                'uses_retrieval': response['uses_retrieval'],
                'num_evidence_sets': len(evidence),
                'num_retrieved_docs': len(response.get('retrieved_docs', []))
            })
            
            if (i + 1) % 10 == 0:
                logger.info(f"Processed {i + 1}/{len(ds)} FEVER samples")
                
        except Exception as e:
            logger.error(f"Error processing FEVER item {i}: {e}")
            continue
    
    logger.info(f"
