#!/usr/bin/env python3
"""
TIRESRAG-R1 Quick Start Evaluation Script
Evaluates TIRESRAG-R1 model on 6 benchmarks from the paper
Uses OpenRLHF framework - evaluation only, no training
"""

import json
import time
import os
import logging
import requests
from datasets import load_dataset
import numpy as np
import re
import string
from collections import Counter
from typing import List, Dict, Any, Optional
import torch
from transformers import AutoTokenizer, AutoModelForCausalLM

# Setup logging
logging.basicConfig(level=logging.INFO, format='%(asctime)s - %(levelname)s - %(message)s')
logger = logging.getLogger(__name__)

class TIRESRAGModel:
    """
    TIRESRAG-R1 model implementation following the OpenRLHF framework
    Implements the think-retrieve-reflect process
    """
    
    def __init__(self, 
                 model_path: str = "your-tiresrag-model-path",
                 retrieval_api_url: str = "http://localhost:8080/retrieve",
                 answer_reflection_url: str = "http://localhost:8081/reflect",
                 sufficient_thinking_url: str = "http://localhost:8082/think",
                 max_tokens: int = 512,
                 temperature: float = 0.1):
        """
        Initialize TIRESRAG-R1 model
        
        Args:
            model_path: Path to the TIRESRAG-R1 model
            retrieval_api_url: URL for retrieval service
            answer_reflection_url: URL for answer reflection reward service
            sufficient_thinking_url: URL for sufficiency/thinking reward service
            max_tokens: Maximum tokens to generate
            temperature: Sampling temperature
        """
        self.model_path = model_path
        self.retrieval_api_url = retrieval_api_url
        self.answer_reflection_url = answer_reflection_url
        self.sufficient_thinking_url = sufficient_thinking_url
        self.max_tokens = max_tokens
        self.temperature = temperature
        
        logger.info(f"Initializing TIRESRAG-R1 model from {model_path}")
        
        try:
            # Load tokenizer and model
            self.tokenizer = AutoTokenizer.from_pretrained(model_path)
            self.model = AutoModelForCausalLM.from_pretrained(
                model_path,
                torch_dtype=torch.float16,
                device_map="auto"
            )
            
            # Set pad token if not exists
            if self.tokenizer.pad_token is None:
                self.tokenizer.pad_token = self.tokenizer.eos_token
            
            logger.info("TIRESRAG-R1 model loaded successfully")
            
        except Exception as e:
            logger.error(f"Failed to load TIRESRAG-R1 model: {e}")
            raise
    
    def retrieve_documents(self, query: str, top_k: int = 5) -> List[Dict[str, Any]]:
        """
        Retrieve documents using the retrieval API service
        
        Args:
            query: Search query
            top_k: Number of documents to retrieve
            
        Returns:
            List of retrieved documents with scores
        """
        try:
            response = requests.post(
                self.retrieval_api_url,
                json={"query": query, "top_k": top_k},
                timeout=30
            )
            
            if response.status_code == 200:
                return response.json().get("documents", [])
            else:
                logger.warning(f"Retrieval API returned status {response.status_code}")
                return []
                
        except requests.exceptions.RequestException as e:
            logger.warning(f"Retrieval API request failed: {e}")
            # Fallback to empty retrieval
            return []
    
    def get_answer_reflection_reward(self, query: str, answer: str, context: str = "") -> float:
        """
        Get answer reflection reward from the reward service
        
        Args:
            query: The question
            answer: Generated answer
            context: Retrieved context
            
        Returns:
            Reflection reward score
        """
        try:
            response = requests.post(
                self.answer_reflection_url,
                json={
                    "query": query,
                    "answer": answer,
                    "context": context
                },
                timeout=10
            )
            
            if response.status_code == 200:
                return response.json().get("reward", 0.0)
            else:
                return 0.0
                
        except requests.exceptions.RequestException:
            return 0.0
    
    def get_sufficiency_thinking_reward(self, query: str, thinking_process: str) -> Dict[str, float]:
        """
        Get sufficiency and thinking quality rewards
        
        Args:
            query: The question
            thinking_process: The thinking/reasoning process
            
        Returns:
            Dictionary with sufficiency and thinking rewards
        """
        try:
            response = requests.post(
                self.sufficient_thinking_url,
                json={
                    "query": query,
                    "thinking": thinking_process
                },
                timeout=10
            )
            
            if response.status_code == 200:
                data = response.json()
                return {
                    "sufficiency": data.get("sufficiency_reward", 0.0),
                    "thinking": data.get("thinking_reward", 0.0)
                }
            else:
                return {"sufficiency": 0.0, "thinking": 0.0}
                
        except requests.exceptions.RequestException:
            return {"sufficiency": 0.0, "thinking": 0.0}
    
    def format_tiresrag_prompt(self, query: str, retrieved_docs: List[Dict] = None) -> str:
        """
        Format prompt following TIRESRAG-R1 think-retrieve-reflect structure
        
        Args:
            query: The question
            retrieved_docs: Retrieved documents
            
        Returns:
            Formatted prompt
        """
        prompt = f"Question: {query}\n\n"
        
        # Add retrieved context if available
        if retrieved_docs:
            prompt += "Retrieved Context:\n"
            for i, doc in enumerate(retrieved_docs[:5], 1):
                content = doc.get('content', doc.get('text', ''))
                title = doc.get('title', f'Document {i}')
                prompt += f"[{i}] {title}: {content[:500]}...\n"
            prompt += "\n"
        
        # TIRESRAG-R1 specific instructions
        prompt += """Think step by step and provide a comprehensive answer:

1. **Think**: Analyze what information is needed to answer this question
2. **Retrieve**: Use the provided context to find relevant information  
3. **Reflect**: Evaluate the sufficiency and accuracy of your reasoning

Answer:"""
        
        return prompt
    
    def generate_response(self, query: str, max_retrieval_rounds: int = 2) -> Dict[str, Any]:
        """
        Generate response using TIRESRAG-R1's think-retrieve-reflect process
        
        Args:
            query: The question to answer
            max_retrieval_rounds: Maximum rounds of retrieval
            
        Returns:
            Dictionary containing response and metadata
        """
        start_time = time.time()
        
        # Step 1: Initial thinking
        initial_thinking_prompt = f"Question: {query}\n\nThink about what information you need to answer this question:\n"
        
        thinking_inputs = self.tokenizer(initial_thinking_prompt, return_tensors="pt", truncation=True)
        
        with torch.no_grad():
            thinking_outputs = self.model.generate(
                **thinking_inputs,
                max_new_tokens=100,
                temperature=self.temperature,
                do_sample=True,
                pad_token_id=self.tokenizer.eos_token_id
            )
        
        thinking_process = self.tokenizer.decode(
            thinking_outputs[0][thinking_inputs['input_ids'].shape[1]:], 
            skip_special_tokens=True
        ).strip()
        
        # Step 2: Retrieve documents
        retrieved_docs = self.retrieve_documents(query, top_k=5)
        
        # Step 3: Generate answer with retrieved context
        full_prompt = self.format_tiresrag_prompt(query, retrieved_docs)
        
        inputs = self.tokenizer(full_prompt, return_tensors="pt", truncation=True, max_length=2048)
        
        with torch.no_grad():
            outputs = self.model.generate(
                **inputs,
                max_new_tokens=self.max_tokens,
                temperature=self.temperature,
                do_sample=True,
                pad_token_id=self.tokenizer.eos_token_id
            )
        
        response = self.tokenizer.decode(
            outputs[0][inputs['input_ids'].shape[1]:], 
            skip_special_tokens=True
        ).strip()
        
        # Step 4: Reflection and reward computation
        context_text = "\n".join([doc.get('content', doc.get('text', ''))[:200] for doc in retrieved_docs])
        
        reflection_reward = self.get_answer_reflection_reward(query, response, context_text)
        thinking_rewards = self.get_sufficiency_thinking_reward(query, thinking_process)
        
        inference_time = time.time() - start_time
        
        return {
            "response": response,
            "thinking_process": thinking_process,
            "retrieved_docs": retrieved_docs,
            "reflection_reward": reflection_reward,
            "sufficiency_reward": thinking_rewards["sufficiency"],
            "thinking_reward": thinking_rewards["thinking"],
            "inference_time": inference_time,
            "num_retrieved": len(retrieved_docs)
        }

class RAGEvaluator:
    """Evaluator for RAG systems following standard QA evaluation metrics"""
    
    def __init__(self):
        pass
    
    def normalize_answer(self, s):
        """Normalize answer for evaluation (from SQuAD evaluation)"""
        def remove_articles(text):
            regex = re.compile(r'\b(a|an|the)\b', re.IGNORECASE)
            return re.sub(regex, ' ', text)
        
        def white_space_fix(text):
            return ' '.join(text.split())
        
        def remove_punc(text):
            exclude = set(string.punctuation)
            return ''.join(ch for ch in text if ch not in exclude)
        
        def lower(text):
            return text.lower()
        
        return white_space_fix(remove_articles(remove_punc(lower(s))))
    
    def exact_match_score(self, prediction, ground_truth):
        """Compute exact match score"""
        return (self.normalize_answer(prediction) == self.normalize_answer(ground_truth))
    
    def f1_score(self, prediction, ground_truth):
        """Compute F1 score (token-level)"""
        pred_tokens = self.normalize_answer(prediction).split()
        gold_tokens = self.normalize_answer(ground_truth).split()
        
        if not pred_tokens and not gold_tokens:
            return 1.0
        if not pred_tokens or not gold_tokens:
            return 0.0
        
        common = Counter(pred_tokens) & Counter(gold_tokens)
        num_same = sum(common.values())
        
        if num_same == 0:
            return 0.0
        
        precision = 1.0 * num_same / len(pred_tokens)
        recall = 1.0 * num_same / len(gold_tokens)
        f1 = (2 * precision * recall) / (precision + recall)
        
        return f1

    def evaluate_multiple_answers(self, prediction, ground_truths):
        """Evaluate against multiple possible ground truth answers"""
        if not ground_truths:
            return {'em': 0.0, 'f1': 0.0}
        
        best_em = 0.0
        best_f1 = 0.0
        
        for gt in ground_truths:
            if not gt or not gt.strip():
                continue
                
            em = self.exact_match_score(prediction, gt)
            f1 = self.f1_score(prediction, gt)
            
            best_em = max(best_em, em)
            best_f1 = max(best_f1, f1)
        
        return {'em': best_em, 'f1': best_f1}

def run_hotpot_qa_benchmark(model, evaluator, sample_size: int = 100):
    """HotpotQA benchmark - multi-hop reasoning"""
    logger.info(f"Running HotpotQA benchmark with {sample_size} samples...")
    
    try:
        ds = load_dataset("hotpot_qa", "distractor", split="validation")
        if sample_size < len(ds):
            ds = ds.select(range(sample_size))
        
        results = []
        
        for i, item in enumerate(ds):
            try:
                question = item.get('question', '')
                answer = item.get('answer', '')
                level = item.get('level', 'unknown')
                type_question = item.get('type', 'unknown')
                
                # Generate response using TIRESRAG-R1
                result = model.generate_response(question)
                response_text = result["response"]
                
                # Evaluation
                scores = evaluator.evaluate_multiple_answers(response_text, [answer]) if answer else {'em': 0.0, 'f1': 0.0}
                
                results.append({
                    'dataset': 'hotpot_qa',
                    'question': question,
                    'response': response_text,
                    'ground_truth_answer': answer,
                    'level': level,
                    'type': type_question,
                    'exact_match': scores['em'],
                    'f1_score': scores['f1'],
                    'inference_time': result['inference_time'],
                    'reflection_reward': result['reflection_reward'],
                    'sufficiency_reward': result['sufficiency_reward'],
                    'thinking_reward': result['thinking_reward'],
                    'num_retrieved': result['num_retrieved'],
                    'thinking_process': result['thinking_process'][:200]  # Truncate for storage
                })
                
                if (i + 1) % 10 == 0:
                    logger.info(f"Processed {i + 1}/{len(ds)} HotpotQA samples")
                    
            except Exception as e:
                logger.error(f"Error processing HotpotQA item {i}: {e}")
                continue
        
        return results
        
    except Exception as e:
        logger.error(f"Error running HotpotQA: {e}")
        return []

def run_natural_questions_benchmark(model, evaluator, sample_size: int = 100):
    """Natural Questions benchmark"""
    logger.info(f"Running Natural Questions benchmark with {sample_size} samples...")
    
    try:
        ds = load_dataset("natural_questions", "default", split="validation")
        if sample_size < len(ds):
            ds = ds.select(range(sample_size))
        
        results = []
        
        for i, item in enumerate(ds):
            try:
                question = item.get('question', {}).get('text', '')
                annotations = item.get('annotations', [])
                
                # Extract answer texts
                answer_texts = []
                if annotations:
                    for ann in annotations:
                        short_answers = ann.get('short_answers', [])
                        for sa in short_answers:
                            text = sa.get('text', '')
                            if text and text.strip():
                                answer_texts.append(text.strip())
                
                # Generate response
                result = model.generate_response(question)
                response_text = result["response"]
                
                # Evaluation
                scores = evaluator.evaluate_multiple_answers(response_text, answer_texts) if answer_texts else {'em': 0.0, 'f1': 0.0}
                
                results.append({
                    'dataset': 'natural_questions',
                    'question': question,
                    'response': response_text,
                    'ground_truth_answers': answer_texts,
                    'exact_match': scores['em'],
                    'f1_score': scores['f1'],
                    'inference_time': result['inference_time'],
                    'reflection_reward': result['reflection_reward'],
                    'sufficiency_reward': result['sufficiency_reward'],
                    'thinking_reward': result['thinking_reward'],
                    'num_retrieved': result['num_retrieved']
                })
                
                if (i + 1) % 10 == 0:
                    logger.info(f"Processed {i + 1}/{len(ds)} Natural Questions samples")
                    
            except Exception as e:
                logger.error(f"Error processing Natural Questions item {i}: {e}")
                continue
        
        return results
        
    except Exception as e:
        logger.error(f"Error running Natural Questions: {e}")
        return []

def run_squad_v2_benchmark(model, evaluator, sample_size: int = 100):
    """SQuAD v2 benchmark"""
    logger.info(f"Running SQuAD v2 benchmark with {sample_size} samples...")
    
    try:
        ds = load_dataset("rajpurkar/squad_v2", split="validation")
        if sample_size < len(ds):
            ds = ds.select(range(sample_size))
        
        results = []
        
        for i, item in enumerate(ds):
            try:
                question = item.get('question', '')
                answers = item.get('answers', {})
                
                # Extract answers
                answer_texts = answers.get('text', []) if answers else []
                is_impossible = len(answer_texts) == 0
                
                # Generate response
                result = model.generate_response(question)
                response_text = result["response"]
                
                # Evaluation
                if not is_impossible and answer_texts:
                    scores = evaluator.evaluate_multiple_answers(response_text, answer_texts)
                elif is_impossible:
                    no_answer_indicators = ["no answer", "cannot answer", "not provided", "unknown", "unanswerable"]
                    detected_impossible = any(indicator in response_text.lower() for indicator in no_answer_indicators)
                    scores = {'em': 1.0 if detected_impossible else 0.0, 'f1': 1.0 if detected_impossible else 0.0}
                else:
                    scores = {'em': 0.0, 'f1': 0.0}
                
                results.append({
                    'dataset': 'squad_v2',
                    'question': question,
                    'response': response_text,
                    'ground_truth_answers': answer_texts,
                    'is_impossible': is_impossible,
                    'exact_match': scores['em'],
                    'f1_score': scores['f1'],
                    'inference_time': result['inference_time'],
                    'reflection_reward': result['reflection_reward'],
                    'sufficiency_reward': result['sufficiency_reward'],
                    'thinking_reward': result['thinking_reward'],
                    'num_retrieved': result['num_retrieved']
                })
                
                if (i + 1) % 10 == 0:
                    logger.info(f"Processed {i + 1}/{len(ds)} SQuAD v2 samples")
                    
            except Exception as e:
                logger.error(f"Error processing SQuAD v2 item {i}: {e}")
                continue
        
        return results
        
    except Exception as e:
        logger.error(f"Error running SQuAD v2: {e}")
        return []

def run_trivia_qa_benchmark(model, evaluator, sample_size: int = 100):
    """TriviaQA benchmark"""
    logger.info(f"Running TriviaQA benchmark with {sample_size} samples...")
    
    try:
        ds = load_dataset("trivia_qa", "rc", split="validation")
        if sample_size < len(ds):
            ds = ds.select(range(sample_size))
        
        results = []
        
        for i, item in enumerate(ds):
            try:
                question = item.get('question', '')
                answer = item.get('answer', {})
                
                # Extract answers
                answer_texts = []
                if answer:
                    value = answer.get('value', '')
                    aliases = answer.get('aliases', [])
                    if value:
                        answer_texts.append(value)
                    answer_texts.extend(aliases)
                
                # Generate response
                result = model.generate_response(question)
                response_text = result["response"]
                
                # Evaluation
                scores = evaluator.evaluate_multiple_answers(response_text, answer_texts) if answer_texts else {'em': 0.0, 'f1': 0.0}
                
                results.append({
                    'dataset': 'trivia_qa',
                    'question': question,
                    'response': response_text,
                    'ground_truth_answers': answer_texts,
                    'exact_match': scores['em'],
                    'f1_score': scores['f1'],
                    'inference_time': result['inference_time'],
                    'reflection_reward': result['reflection_reward'],
                    'sufficiency_reward': result['sufficiency_reward'],
                    'thinking_reward': result['thinking_reward'],
                    'num_retrieved': result['num_retrieved']
                })
                
                if (i + 1) % 10 == 0:
                    logger.info(f"Processed {i + 1}/{len(ds)} TriviaQA samples")
                    
            except Exception as e:
                logger.error(f"Error processing TriviaQA item {i}: {e}")
                continue
        
        return results
        
    except Exception as e:
        logger.error(f"Error running TriviaQA: {e}")
        return []

def run_crag_benchmark(model, evaluator, sample_size: int = 100):
    """CRAG benchmark"""
    logger.info(f"Running CRAG benchmark with {sample_size} samples...")
    
    try:
        ds = load_dataset("facebook/crag", split="dev", trust_remote_code=True)
        if sample_size < len(ds):
            ds = ds.select(range(sample_size))
        
        results = []
        
        for i, item in enumerate(ds):
            try:
                query = item.get('query', '')
                answer = item.get('answer', '')
                alt_ans = item.get('alt_ans', []) or []
                domain = item.get('domain', 'unknown')
                question_type = item.get('question_type', 'unknown')
                
                # Generate response
                result = model.generate_response(query)
                response_text = result["response"]
                
                # Evaluation with multiple ground truths
                ground_truths = [answer] + alt_ans if answer else alt_ans
                ground_truths = [gt for gt in ground_truths if gt and gt.strip()]
                
                scores = evaluator.evaluate_multiple_answers(response_text, ground_truths) if ground_truths else {'em': 0.0, 'f1': 0.0}
                
                results.append({
                    'dataset': 'crag',
                    'query': query,
                    'response': response_text,
                    'ground_truth': answer,
                    'alt_answers': alt_ans,
                    'domain': domain,
                    'question_type': question_type,
                    'exact_match': scores['em'],
                    'f1_score': scores['f1'],
                    'inference_time': result['inference_time'],
                    'reflection_reward': result['reflection_reward'],
                    'sufficiency_reward': result['sufficiency_reward'],
                    'thinking_reward': result['thinking_reward'],
                    'num_retrieved': result['num_retrieved']
                })
                
                if (i + 1) % 10 == 0:
                    logger.info(f"Processed {i + 1}/{len(ds)} CRAG samples")
                
            except Exception as e:
                logger.error(f"Error processing CRAG item {i}: {e}")
                continue
        
        return results
        
    except Exception as e:
        logger.error(f"Error running CRAG benchmark: {e}")
        return []

def run_ragbench_benchmark(model, evaluator, sample_size: int = 100):
    """RAGBench benchmark (using MS MARCO as proxy)"""
    logger.info(f"Running RAGBench benchmark with {sample_size} samples...")
    
    try:
        ds = load_dataset("ms_marco", "v2.1", split="validation")
        if sample_size < len(ds):
            ds = ds.select(range(sample_size))
        
        results = []
        
        for i, item in enumerate(ds):
            try:
                query = item.get('query', '')
                answers = item.get('answers', [])
                wellFormedAnswers = item.get('wellFormedAnswers', [])
                
                # Get answer texts
                answer_texts = answers + wellFormedAnswers
                answer_texts = [ans for ans in answer_texts if ans and ans.strip()]
                
                # Generate response
                result = model.generate_response(query)
                response_text = result["response"]
                
                # Evaluation
                scores = evaluator.evaluate_multiple_answers(response_text, answer_texts) if answer_texts else {'em': 0.0, 'f1': 0.0}
                
                results.append({
                    'dataset': 'ragbench',
                    'query': query,
                    'response': response_text,
                    'ground_truth_answers': answer_texts,
                    'exact_match': scores['em'],
                    'f1_score': scores['f1'],
                    'inference_time': result['inference_time'],
                    'reflection_reward': result['reflection_reward'],
                    'sufficiency_reward': result['sufficiency_reward'],
                    'thinking_reward': result['thinking_reward'],
                    'num_retrieved': result['num_retrieved']
                })
                
                if (i + 1) % 10 == 0:
                    logger.info(f"Processed {i + 1}/{len(ds)} RAGBench samples")
                    
            except Exception as e:
                logger.error(f"Error processing RAGBench item {i}: {e}")
                continue
        
        return results
        
    except Exception as e:
        logger.error(f"Error running RAGBench: {e}")
        return []

def compute_aggregate_metrics(results):
    """Compute aggregate metrics including TIRESRAG-R1 specific metrics"""
    if not results:
        return {}
    
    # Standard QA metrics
    em_scores = [r['exact_match'] for r in results if 'exact_match' in r]
    f1_scores = [r['f1_score'] for r in results if 'f1_score' in r]
    
    # TIRESRAG-R1 specific metrics
    reflection_rewards = [r['reflection_reward'] for r in results if 'reflection_reward' in r]
    sufficiency_rewards = [r['sufficiency_reward'] for r in results if 'sufficiency_reward' in r]
    thinking_rewards = [r['thinking_reward'] for r in results if 'thinking_reward' in r]
    
    aggregated = {}
    
    if em_scores:
        aggregated['exact_match'] = {
            'mean': float(np.mean(em_scores)),
            'std': float(np.std(em_scores)),
            'count': len(em_scores)
        }
    
    if f1_scores:
        aggregated['f1_score'] = {
            'mean': float(np.mean(f1_scores)),
            'std': float(np.std(f1_scores)),
            'count': len(f1_scores)
        }
    
    # TIRESRAG-R1 rewards
    if reflection_rewards:
        aggregated['reflection_reward'] = {
            'mean': float(np.mean(reflection_rewards)),
            'std': float(np.std(reflection_rewards)),
            'count': len(reflection_rewards)
        }
    
    if sufficiency_rewards:
        aggregated['sufficiency_reward'] = {
            'mean': float(np.mean(sufficiency_rewards)),
            'std': float(np.std(sufficiency_rewards)),
            'count': len(sufficiency_rewards)
        }
    
    if thinking_rewards:
        aggregated['thinking_reward'] = {
            'mean': float(np.mean(thinking_rewards)),
            'std': float(np.std(thinking_rewards)),
            'count': len(thinking_rewards)
        }
    
    return aggregated

def main():
    """Main evaluation function"""
    print("=" * 80)
    print("TIRESRAG-R1 EVALUATION - 6 BENCHMARK DATASETS")
    print("Think-Retrieve-Reflect RAG Evaluation (No Training)")
    print("=" * 80)
    
    # Configuration - Update these paths for your setup
    model_config = {
        "model_path": "your-tiresrag-r1-model-path",  # Update this
        "retrieval_api_url": "http://localhost:8080/retrieve",
        "answer_reflection_url": "http://localhost:8081/reflect", 
        "sufficient_thinking_url": "http://localhost:8082/think",
        "max_tokens": 512,
        "temperature": 0.1
    }
    
    # Initialize TIRESRAG-R1 model
    logger.info("Initializing TIRESRAG-R1 model...")
    try:
        model = TIRESRAGModel(**model_config)
        logger.info("TIRESRAG-R1 model initialized successfully")
    except Exception as e:
        logger.error(f"Failed to initialize TIRESRAG-R1 model: {e}")
        logger.error("Make sure:")
        logger.error("1. Model path is correct")
        logger.error("2. Retrieval and reward API services are running")
        logger.error("3. You have sufficient GPU memory")
        return
    
    # Initialize evaluator
    evaluator = RAGEvaluator()
    
    # Configuration
    sample_size = 100  # Adjust based on your compute resources
    
    # Define the 6 benchmarks
    benchmarks = [
        ("HotpotQA", run_hotpot_qa_benchmark),
        ("Natural Questions", run_natural_questions_benchmark),
        ("SQuAD v2", run_squad_v2_benchmark),
        ("TriviaQA", run_trivia_qa_benchmark),
        ("CRAG", run_crag_benchmark),
        ("RAGBench", run_ragbench_benchmark)
    ]
    
    results = {}
    
    # Run each benchmark
    for benchmark_name, benchmark_func in benchmarks:
        print(f"\n{'=' * 60}")
        print(f"RUNNING: {benchmark_name}")
        print(f"{'=' * 60}")
        
        try:
            start_time = time.time()
            benchmark_results = benchmark_func(model, evaluator, sample_size=sample_size)
            end_time = time.time()
            
            if benchmark_results:
                aggregated = compute_aggregate_metrics(benchmark_results)
                
                results[benchmark_name.lower().replace(' ', '_')] = {
                    'individual_results': benchmark_results,
                    'aggregated_metrics': aggregated,
                    'total_samples': len(benchmark_results),
                    'execution_time': end_time - start_time
                }
                
                logger.info(f"{benchmark_name} completed successfully:")
                logger.info(f"   Samples processed: {len(benchmark_results)}")
                logger.info(f"   Execution time: {end_time - start_time:.2f}s")
                
                if 'exact_match' in aggregated:
                    em_stats = aggregated['exact_match']
                    logger.info(f"   Exact Match: {em_stats['mean']:.3f} ± {em_stats['std']:.3f}")
                
                if 'f1_score' in aggregated:
                    f1_stats = aggregated['f1_score']
                    logger.info(f"   F1 Score: {f1_stats['mean']:.3f} ± {f1_stats['std']:.3f}")
                
                # TIRESRAG-R1 specific metrics
                if 'reflection_reward' in aggregated:
                    ref_stats = aggregated['reflection_reward']
                    logger.info(f"   Reflection Reward: {ref_stats['mean']:.3f} ± {ref_stats['std']:.3f}")
                
                if 'sufficiency_reward' in aggregated:
                    suf_stats = aggregated['sufficiency_reward']
                    logger.info(f"   Sufficiency Reward: {suf_stats['mean']:.3f} ± {suf_stats['std']:.3f}")
                
                if 'thinking_reward' in aggregated:
                    think_stats = aggregated['thinking_reward']
                    logger.info(f"   Thinking Reward: {think_stats['mean']:.3f} ± {think_stats['std']:.3f}")
                    
            else:
                logger.warning(f"{benchmark_name} returned no results")
                results[benchmark_name.lower().replace(' ', '_')] = {
                    'individual_results': [],
                    'aggregated_metrics': {},
                    'total_samples': 0,
                    'execution_time': end_time - start_time,
                    'status': 'failed'
                }
                
        except Exception as e:
            logger.error(f"Error running {benchmark_name}: {e}")
            results[benchmark_name.lower().replace(' ', '_')] = {
                'status': 'error',
                'error_message': str(e)
            }
    
    # Save results
    output_file = f"tiresrag_r1_evaluation_results_{int(time.time())}.json"
    with open(output_file, 'w') as f:
        json.dump(results, f, indent=2, ensure_ascii=False)
    
    logger.info(f"Results saved to {output_file}")
    
    # Print comprehensive summary
    print("\n" + "=" * 80)
    print("TIRESRAG-R1 EVALUATION COMPLETE - FINAL SUMMARY")
    print("=" * 80)
    
    total_samples = 0
    successful_benchmarks = 0
    
    # Calculate overall statistics
    all_em_scores = []
    all_f1_scores = []
    all_reflection_rewards = []
    all_sufficiency_rewards = []
    all_thinking_rewards = []
    
    for benchmark_key, benchmark_data in results.items():
        benchmark_name = benchmark_key.upper().replace('_', ' ')
        total_samples += benchmark_data.get('total_samples', 0)
        
        if benchmark_data.get('total_samples', 0) > 0:
            successful_benchmarks += 1
            print(f"\n{benchmark_name}:")
            
            aggregated = benchmark_data.get('aggregated_metrics', {})
            
            # Core QA metrics
            if 'exact_match' in aggregated:
                em = aggregated['exact_match']
                print(f"   Exact Match: {em['mean']:.3f} ± {em['std']:.3f}")
                all_em_scores.extend([r['exact_match'] for r in benchmark_data['individual_results']])
            
            if 'f1_score' in aggregated:
                f1 = aggregated['f1_score']
                print(f"   F1 Score: {f1['mean']:.3f} ± {f1['std']:.3f}")
                all_f1_scores.extend([r['f1_score'] for r in benchmark_data['individual_results']])
            
            # TIRESRAG-R1 specific metrics
            if 'reflection_reward' in aggregated:
                ref = aggregated['reflection_reward']
                print(f"   Reflection Reward: {ref['mean']:.3f} ± {ref['std']:.3f}")
                all_reflection_rewards.extend([r['reflection_reward'] for r in benchmark_data['individual_results']])
            
            if 'sufficiency_reward' in aggregated:
                suf = aggregated['sufficiency_reward']
                print(f"   Sufficiency Reward: {suf['mean']:.3f} ± {suf['std']:.3f}")
                all_sufficiency_rewards.extend([r['sufficiency_reward'] for r in benchmark_data['individual_results']])
            
            if 'thinking_reward' in aggregated:
                think = aggregated['thinking_reward']
                print(f"   Thinking Reward: {think['mean']:.3f} ± {think['std']:.3f}")
                all_thinking_rewards.extend([r['thinking_reward'] for r in benchmark_data['individual_results']])
            
            print(f"   Execution Time: {benchmark_data.get('execution_time', 0):.2f}s")
            
        else:
            status = benchmark_data.get('status', 'unknown')
            print(f"\n{benchmark_name}: {status}")
            if 'error_message' in benchmark_data:
                print(f"   Error: {benchmark_data['error_message']}")
    
    # Overall statistics across all benchmarks
    print(f"\n" + "=" * 80)
    print("OVERALL STATISTICS ACROSS ALL BENCHMARKS:")
    print("=" * 80)
    
    if all_em_scores:
        print(f"Overall Exact Match: {np.mean(all_em_scores):.3f} ± {np.std(all_em_scores):.3f}")
    
    if all_f1_scores:
        print(f"Overall F1 Score: {np.mean(all_f1_scores):.3f} ± {np.std(all_f1_scores):.3f}")
    
    if all_reflection_rewards:
        print(f"Overall Reflection Reward: {np.mean(all_reflection_rewards):.3f} ± {np.std(all_reflection_rewards):.3f}")
    
    if all_sufficiency_rewards:
        print(f"Overall Sufficiency Reward: {np.mean(all_sufficiency_rewards):.3f} ± {np.std(all_sufficiency_rewards):.3f}")
    
    if all_thinking_rewards:
        print(f"Overall Thinking Reward: {np.mean(all_thinking_rewards):.3f} ± {np.std(all_thinking_rewards):.3f}")
    
    print(f"\nSuccessful benchmarks: {successful_benchmarks}/6")
    print(f"Total samples processed: {total_samples}")
    print(f"Results saved to: {output_file}")
    
    # Performance comparison note
    print("\n" + "=" * 80)
    print("PERFORMANCE COMPARISON NOTES:")
    print("=" * 80)
    print("Compare these results with the paper benchmarks:")
    print("- HotpotQA EM: TIRESRAG-R1-Base (41.0%), TIRESRAG-R1-Instruct (41.0%)")
    print("- 2Wiki EM: TIRESRAG-R1-Base (52.4%), TIRESRAG-R1-Instruct (52.8%)")
    print("- MuSiQue EM: TIRESRAG-R1-Base (16.2%), TIRESRAG-R1-Instruct (19.4%)")
    print("- Bamboogle EM: TIRESRAG-R1-Base (40.4%), TIRESRAG-R1-Instruct (44.0%)")
    
    if successful_benchmarks == 6:
        print("\nAll benchmarks completed successfully!")
        print("Results ready for comparison with TIRESRAG-R1 paper metrics")
    else:
        print(f"\n{6 - successful_benchmarks} benchmark(s) failed - check logs for details")
    
    print("\nTIRESRAG-R1 evaluation complete!")
    
    return results

def check_prerequisites():
    """Check if all required services and dependencies are available"""
    print("Checking prerequisites for TIRESRAG-R1 evaluation...")
    
    # Check GPU
    if torch.cuda.is_available():
        print(f"GPU Available: {torch.cuda.get_device_name(0)}")
        print(f"GPU Memory: {torch.cuda.get_device_properties(0).total_memory / 1e9:.1f}GB")
    else:
        print("WARNING: No GPU detected - evaluation will be slow")
    
    # Check API services
    services = [
        ("Retrieval API", "http://localhost:8080/retrieve"),
        ("Answer Reflection API", "http://localhost:8081/reflect"),
        ("Sufficiency/Thinking API", "http://localhost:8082/think")
    ]
    
    for service_name, url in services:
        try:
            response = requests.get(url.replace('/retrieve', '/health').replace('/reflect', '/health').replace('/think', '/health'), timeout=5)
            if response.status_code == 200:
                print(f"{service_name}: OK")
            else:
                print(f"{service_name}: WARNING - Service responded but may not be ready")
        except requests.exceptions.RequestException:
            print(f"{service_name}: FAILED - Service not available at {url}")
            print(f"  Make sure to start the service as described in the README")

if __name__ == "__main__":
    print("TIRESRAG-R1 EVALUATION SYSTEM")
    print("Think-Retrieve-Reflect RAG Evaluation")
    print("=" * 70)
    
    # Check prerequisites
    check_prerequisites()
    
    print("\nStarting TIRESRAG-R1 evaluation...")
    print("Make sure you have:")
    print("1. Updated model_path in the script")
    print("2. Started retrieval API service (bash wiki_servish.sh)")
    print("3. Started reward API services:")
    print("   - bash answer_reflection_reward.sh")
    print("   - bash sufficient_thinking_reward.sh")
    
    try:
        results = main()
        print("\nEVALUATION COMPLETED SUCCESSFULLY!")
        print("Results include both standard QA metrics (EM, F1) and")
        print("TIRESRAG-R1 specific metrics (reflection, sufficiency, thinking rewards)")
        
    except KeyboardInterrupt:
        print("\nEvaluation interrupted by user")
    except Exception as e:
        logger.error(f"Fatal error during evaluation: {e}")
        print("EVALUATION FAILED - check logs above for details")
        print("\nCommon issues:")
        print("- Model path not updated")
        print("- API services not running")
        print("- Insufficient GPU memory")
        print("- Network issues downloading datasets")
